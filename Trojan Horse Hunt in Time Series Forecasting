{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":101039,"databundleVersionId":12513485,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#All library\n\nimport numpy as np \nimport pandas as pd \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/trojan-horse-hunt-in-space/clean_train_data.csv')\ndf1=pd.read_csv ('/kaggle/input/trojan-horse-hunt-in-space/sample_submission_solution.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Set random seeds for reproducibility\ntorch.manual.seed(42)\nnp.random.seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TimeSeriesDataset(Dataset):\n    #Dataset class for time series data\n    def __init__(self, data, sequence_length=100, forecast_horizon=75):\n        self.data = torch.FloatTensor(data)\n        self.seq_len = sequence_length\n        self.forecast_len = forecast_horizon\n        \n    def __len__(self):\n        return len(self.data) - self.seq_len - self.forecast_len + 1\n    \n    def __getitem__(self, idx):\n        x = self.data[idx:idx + self.seq_len]\n        y = self.data[idx + self.seq_len:idx + self.seq_len + self.forecast_len]\n        return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:40:48.442893Z","iopub.execute_input":"2025-08-21T18:40:48.443223Z","iopub.status.idle":"2025-08-21T18:40:48.448651Z","shell.execute_reply.started":"2025-08-21T18:40:48.443193Z","shell.execute_reply":"2025-08-21T18:40:48.447810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Detecting the trojans in poisoned models\nclass Trojandetector:\n    def __init__(self, clean_model_path, poisoned_model_dir, clean_data_path):\n        self.clean_model_path = clean_model_path\n        self.poisoned_model_dir = poisoned_model_dir\n        self.clean_data_path = clean_data_path\n        # Load the clean data\n        self.clean_data = self.load_clean_data()\n\n        #model parameters\n        self.input_dim = 3 #we only have 3 channels 44 , 45 , 46\n        self.sequence_length = 736000\n        self.forecast_horizon = 75\n\n        #loading the model\n        self.clean_model = self.load_model(clean_model_path)\n        self.poisoned_models = self.load_poisoned_models()\n\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:18:03.596127Z","iopub.execute_input":"2025-08-21T19:18:03.596450Z","iopub.status.idle":"2025-08-21T19:18:03.601742Z","shell.execute_reply.started":"2025-08-21T19:18:03.596410Z","shell.execute_reply":"2025-08-21T19:18:03.600453Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"#Loading and preprocessing the train data\ndef load_clean_data(self) : \n        print(\"Cleaned train data\")\n        clean_data = pd.read.csv(self.clean_data_path)\n\n#extracting the 3 channels\n        channels = ['channel_44', 'channel_45', 'channel_46']\n        data = clean_data[channels].values\n#normalize\n        scaler = StandardScaler()\n        normalized_data = scaler.fit_tranform(clean_data)\n        self.scaler =scaler\n        return normalized_data\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:18:00.910783Z","iopub.execute_input":"2025-08-21T19:18:00.911044Z","iopub.status.idle":"2025-08-21T19:18:00.916115Z","shell.execute_reply.started":"2025-08-21T19:18:00.911025Z","shell.execute_reply":"2025-08-21T19:18:00.914719Z"}},"outputs":[],"execution_count":34}]}